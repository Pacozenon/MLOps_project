{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# About this notebook\n",
        "This notebook was taken and changed from O'Reilly book Hands-on Machine Learning with Scikit-Learn, Keras and TensorFlow.\n",
        "\n",
        "The task is to predict median house values in Californian districts, given a number of features from these districts.\n",
        "\n",
        "This notebook contains all the sample code and solutions to the exercices in [chapter 2](https://github.com/ageron/handson-ml2/blob/master/02_end_to_end_machine_learning_project.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqC7Gc3B9O5k"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0T8lE-eCZ86l",
        "outputId": "c2c469ae-3df7-4d30-f9cb-a27cb7af4e40"
      },
      "outputs": [],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1uSyFRX59KJN"
      },
      "outputs": [],
      "source": [
        "# Python ≥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Scikit-Learn ≥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= '0.20'\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "# Where to save the figures\n",
        "PROJECT_ROOT_DIR = '.'\n",
        "CHAPTER_ID = 'end_to_end_project'\n",
        "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, 'images', CHAPTER_ID)\n",
        "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension='png', resolution=300):\n",
        "    path = os.path.join(IMAGES_PATH, fig_id + '.' + fig_extension)\n",
        "    print('Saving figure', fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format=fig_extension, dpi=resolution)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEWxrkTx941x"
      },
      "source": [
        "# Get the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvH-qCekURDy"
      },
      "outputs": [],
      "source": [
        "!pip freeze"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TguzJjaC9nlB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tarfile\n",
        "import urllib.request\n",
        "\n",
        "DOWNLOAD_ROOT = 'https://raw.githubusercontent.com/ageron/handson-ml2/master/'\n",
        "HOUSING_PATH = os.path.join('datasets','housing')\n",
        "HOUSING_URL = DOWNLOAD_ROOT + 'datasets/housing/housing.tgz'\n",
        "\n",
        "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
        "  if not os.path.isdir(housing_path):\n",
        "    os.makedirs(housing_path)\n",
        "\n",
        "  tgz_path = os.path.join(housing_path, 'housing.tgz')\n",
        "  urllib.request.urlretrieve(housing_url, tgz_path)\n",
        "  housing_tgz = tarfile.open(tgz_path)\n",
        "  housing_tgz.extractall(path=housing_path)\n",
        "  housing_tgz.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1quSmpR2-gqb",
        "outputId": "cac311b7-0f01-4fbf-f675-d24575c4d492"
      },
      "outputs": [],
      "source": [
        "# fetch_housing_data()\n",
        "# print('Data downloaded!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6lGP5Z-AusD"
      },
      "source": [
        "# Explore the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7AH1FYT_Fcj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def load_housing_data(housing_path=HOUSING_PATH):\n",
        "  csv_path = os.path.join(housing_path, 'housing.csv')\n",
        "  return pd.read_csv(csv_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "Q52wrssoBA9k",
        "outputId": "1306cd90-edc6-4aa0-cbef-111411ead152"
      },
      "outputs": [],
      "source": [
        "housing = load_housing_data()\n",
        "housing.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_aNzSdMiBFdh",
        "outputId": "5632cb5e-874c-4414-a082-390c69e5efc4"
      },
      "outputs": [],
      "source": [
        "housing.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3xQTDa2BMFi",
        "outputId": "0f6f7868-7ff8-4c97-be39-7d5a7bda1817"
      },
      "outputs": [],
      "source": [
        "housing['ocean_proximity'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "lxXwtQwmBXti",
        "outputId": "38fc9e3f-1376-4152-df46-dfb8ca7d1c54"
      },
      "outputs": [],
      "source": [
        "housing.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rm7ctPMBHLoR"
      },
      "source": [
        "* 25% of the districts have a housing_median_age lower than 18\n",
        "* 50% are lower than 29\n",
        "* 75% are lower than 37"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "l41QU1sIBc4p",
        "outputId": "77121ace-be3b-4b77-e29a-14d9053c54db"
      },
      "outputs": [],
      "source": [
        "# Showing the number of instances (on the vertical axis) that have a given value range (on the horizontal axis)\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "housing.hist(bins=50, figsize=(20,15))\n",
        "save_fig('attribute_histogram_plots')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZziyoVtRIUyn"
      },
      "source": [
        "Few things to notice in the above histograms:\n",
        "\n",
        "1.   First, the median income attribute does not look like it is expressed in US dollars (USD), but the numbers represent roughly tens of thousands of dollars (e.g., 3 actually means about \\$30,000).\n",
        "\n",
        "2.   The housing meadian age and the median house value were also capped. This may be a serious problem since it is the target attribute (the labels). The ML algorithm may learn that prices never go beyond that limit. If required precise predictions even beyond \\$500,000, the options are:\n",
        "    \n",
        "    a. Collect proper labels for the districts whose labels were capped.\n",
        "    \n",
        "    b. Remove those districts from the training and test set.\n",
        "\n",
        "3.   These attributes have very different scales, is neccesary to apply feature scaling.\n",
        "\n",
        "4.   Many histograms are tail-heavy: they extend much farther to the right of the median than to the left. This may make it a bit harder for some Machine Learning algorithms to detect patterns. Future transformations to these attributes have to be done in order to have more bell-shaped distributions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPFELQLLC_F7"
      },
      "source": [
        "Visualizing the data to have a better overview of how it is distributed with a kernel density estimate (KDE) plot, which is a method for visualizing the distribution of observations in a dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "id": "5Efs0pMcB38k",
        "outputId": "f115dbba-38a9-46c8-b08b-0ca601d3b4b6"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "plt.figure(figsize=(30,40))\n",
        "for i, col in enumerate(housing.columns):\n",
        "    if housing[col].dtype != 'object':\n",
        "        ax = plt.subplot(21, 3, i+1)\n",
        "        sns.kdeplot(housing[col], ax=ax)\n",
        "        plt.xlabel(col, fontsize=10)\n",
        "        plt.tight_layout(pad=2.0)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvUh9H0EDJAw"
      },
      "source": [
        "Then, in order to have a better view of quartiles of the dataset, and the outliers, let's plot some boxes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "l-NROkNiCx28",
        "outputId": "15df064a-9d6a-4c56-aa2a-5659cca40c06"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(30,40))\n",
        "for i, col in enumerate(housing.columns):\n",
        "    if housing[col].dtype != 'object':\n",
        "        ax = plt.subplot(21, 2, i+1)\n",
        "        sns.boxplot(housing[col], ax=ax)\n",
        "        plt.xlabel(col, fontsize=10)\n",
        "        plt.tight_layout(pad=2.0)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFv5_igEDRmT"
      },
      "outputs": [],
      "source": [
        "# to make this notebook's output identical at every run\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJYYozPQDk-L"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# For illustration only. Sklearn has train_test_split() [Implementation from scratch]\n",
        "def split_train_test(data, test_ratio):\n",
        "    shuffled_indices = np.random.permutation(len(data))\n",
        "    test_set_size = int(len(data) * test_ratio)\n",
        "    test_indices = shuffled_indices[:test_set_size]\n",
        "    train_indices = shuffled_indices[test_set_size:]\n",
        "    return data.iloc[train_indices], data.iloc[test_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eWil7kFDpJs",
        "outputId": "87a7f184-70f2-4632-ddf5-ee8f86ba7cdc"
      },
      "outputs": [],
      "source": [
        "train_set, test_set = split_train_test(housing, 0.2)\n",
        "len(train_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q01EQYk_DqXj",
        "outputId": "01d26739-ab25-46a5-c182-654e49bcb27a"
      },
      "outputs": [],
      "source": [
        "len(test_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmZwWnKKOFEc"
      },
      "source": [
        "To ensure that the test set will remain consistent across multiple runs, even if the data is refreshed. The new test set will contain 20% of the new instances, but it will not contain any instance that was previously in the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fuR-eVdmDsDG"
      },
      "outputs": [],
      "source": [
        "from zlib import crc32\n",
        "\n",
        "def test_set_check(identifier, test_ratio):\n",
        "    return crc32(np.int64(identifier)) & 0xffffffff < test_ratio * 2**32\n",
        "\n",
        "def split_train_test_by_id(data, test_ratio, id_column):\n",
        "    ids = data[id_column]\n",
        "    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio))\n",
        "    return data.loc[~in_test_set], data.loc[in_test_set]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGPw-1fqDz6e"
      },
      "outputs": [],
      "source": [
        "import hashlib\n",
        "\n",
        "def test_set_check(identifier, test_ratio, hash=hashlib.md5):\n",
        "    return hash(np.int64(identifier)).digest()[-1] < 256 * test_ratio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYdBBnhMD-vI"
      },
      "source": [
        "If you want an implementation that supports any hash function and is compatible with both Python 2 and Python 3, here is one:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "663zCMNtD7kK"
      },
      "outputs": [],
      "source": [
        "def test_set_check(identifier, test_ratio, hash=hashlib.md5):\n",
        "    return bytearray(hash(np.int64(identifier)).digest())[-1] < 256 * test_ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wihlHDFPEBh7"
      },
      "outputs": [],
      "source": [
        "housing_with_id = housing.reset_index()   # adds an `index` column\n",
        "train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, 'index')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jXUpvqjOe0t"
      },
      "source": [
        "Since the housing dataset does not have an identifier column, the simplest solution is to use index as the ID.\n",
        "\n",
        "If the row index is used as a unique identifier, it's neccesary to make sured that new data gets appended to the end of the dataset and that no row ever gets deleted.\n",
        "\n",
        "If that's not possible, then it can be tried to buils a unique identifier. For example, a district's latitude and longitude are guareanteed to be stable for a few million years."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EID1UPukEGZL"
      },
      "outputs": [],
      "source": [
        "housing_with_id['id'] = housing['longitude'] * 1000 + housing['latitude']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2oqwyxaQUfC"
      },
      "outputs": [],
      "source": [
        "train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, 'id')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "rWmNR1MhEM7D",
        "outputId": "55b1a6f4-93fb-416e-91a7-1ff75cc07dc5"
      },
      "outputs": [],
      "source": [
        "test_set.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07C014xmENa5"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNM4Wea-Qoj-"
      },
      "source": [
        "**Notes:**\n",
        "\n",
        "A purely random sampling method is generally fine if the dataset is large enough (specially relative to the number of attributes), but if it is not, there is a risk of introducing a significant sampling bias. To solver this, a technique called *stratified sampling*: the population is divided into homogeneus subgroups called *strata*, and right number of instances are sampled from each stratum to guarantee the that test set is representative of the overall population. Otherwise, the selection result would be significantly biased."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "YMMSkmfk_4jN",
        "outputId": "695c1024-6b12-4bba-e991-95d741934828"
      },
      "outputs": [],
      "source": [
        "test_set.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2j_ja8RbC2n"
      },
      "source": [
        "It is supposed that the experts in this housing sataset say that the median income is a very important attribute to predict the median housing prices. To ensure that the test set is representative of the various categories of incomes in the whole dataset, an income category attribute  has to be created.\n",
        "\n",
        "It is important to have a sufficient number of instances in the dataset of each stratum, or else the estimate of a stratum's importance may be biased. This means that it should not have too many strata, and each stratum should be large enough."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "0czrbHM4_6C0",
        "outputId": "383bd0ea-83ca-4ede-f085-8454f8b0a5f5"
      },
      "outputs": [],
      "source": [
        "housing['median_income'].hist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BusMyE8ZTyls"
      },
      "source": [
        "The following code uses the pd.cut() function to create an income category attribute with five categories (labeled from 1 to 5): category 1 ranges from 0 to 1.5 (i.e., less than $15,000), category 2 from 1.5 to 3, and so on:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0d49qbtAAQr"
      },
      "outputs": [],
      "source": [
        "housing['income_cat'] = pd.cut(housing['median_income'],\n",
        "                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
        "                               labels=[1, 2, 3, 4, 5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ip7sIpGlBxYF",
        "outputId": "8c6bdaff-377b-4447-8646-473895713358"
      },
      "outputs": [],
      "source": [
        "housing['income_cat'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "UHZaczNCB5oN",
        "outputId": "0f000f0d-0b74-4804-a6a9-0f744c2fd8f6"
      },
      "outputs": [],
      "source": [
        "housing['income_cat'].hist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXtEPtdJCJ96"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bAsqMal8g_u"
      },
      "outputs": [],
      "source": [
        "for train_index, test_index in split.split(housing, housing['income_cat']):\n",
        "    strat_train_set = housing.loc[train_index]\n",
        "    strat_test_set = housing.loc[test_index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAZYRjIiFQOq",
        "outputId": "2c3a5948-a185-40a3-cd6a-c527ad58ffc6"
      },
      "outputs": [],
      "source": [
        "strat_test_set['income_cat'].value_counts() / len(strat_test_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hk-V7SdXFqGO",
        "outputId": "3961e26a-c78b-429c-aff7-3dcebc6aa82b"
      },
      "outputs": [],
      "source": [
        "housing['income_cat'].value_counts() / len(housing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2kLiUYGUHsk"
      },
      "source": [
        "With similar code the income category proportions can be measured in the full dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-OA7bn2T9k7"
      },
      "outputs": [],
      "source": [
        "def income_cat_proportions(data):\n",
        "    return data['income_cat'].value_counts() / len(data)\n",
        "\n",
        "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n",
        "\n",
        "compare_props = pd.DataFrame({\n",
        "    'Overall': income_cat_proportions(housing),\n",
        "    'Stratified': income_cat_proportions(strat_test_set),\n",
        "    'Random': income_cat_proportions(test_set),\n",
        "}).sort_index()\n",
        "compare_props['Rand. %error'] = 100 * compare_props['Random'] / compare_props['Overall'] - 100\n",
        "compare_props['Strat. %error'] = 100 * compare_props['Stratified'] / compare_props['Overall'] - 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "XfdyImctT_t_",
        "outputId": "623c8693-b1fb-4c3e-907b-43cde2d203fb"
      },
      "outputs": [],
      "source": [
        "compare_props"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxFuEIHnURh3"
      },
      "source": [
        "The test set generated using stratified sampling has income category proportions almost identical to those in the full dataset, whereas the test set generated using purely random sampling is skewed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lu9uwOrBqjak"
      },
      "source": [
        "# Discover and Visualize the Data to Gain Insights\n",
        "If the training set is very large, a sample exploration set may be used to explore the data. For this case, the set is quite small, so it can be directly used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPX_gr5Yp9fB"
      },
      "outputs": [],
      "source": [
        "housing = strat_train_set.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uuZCp77rjNT"
      },
      "source": [
        "## Visualizing Geographical Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "GkYSEImRUBjf",
        "outputId": "8a8ec950-1763-4bc7-a015-8f201245e897"
      },
      "outputs": [],
      "source": [
        "# Graphing the geographical information by latitude and longitude and using a scatterplot of all districts\n",
        "housing.plot(kind='scatter', x='longitude', y='latitude')\n",
        "save_fig('bad_visualization_plot')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "N6x_K7N1sB05",
        "outputId": "d01681a7-a40d-4515-f534-c6f307dc791d"
      },
      "outputs": [],
      "source": [
        "# To visualize some particular patterns, the alpha option has to be set to 0.1\n",
        "housing.plot(kind='scatter', x='longitude', y='latitude', alpha=0.1)\n",
        "save_fig('better_visualization_plot')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "id": "a6UHo2Ows0F4",
        "outputId": "92a65a9f-c618-4638-f91a-0ca2e1182765"
      },
      "outputs": [],
      "source": [
        "housing.plot(kind='scatter', x='longitude', y='latitude', alpha=0.05)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBAq6YZn4nri"
      },
      "source": [
        "# Looking for correlations\n",
        "Since the dataser is not too large, the *Standard Correlation Coefficient* (also called Pearson's r) can be easily computed between every pair of attributes using the `corr()` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQrihDWfuNQT"
      },
      "outputs": [],
      "source": [
        "corr_matrix = housing.corr()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAwAzr9H5KXT",
        "outputId": "b352219d-c00d-4b7f-e6a6-1c7652e9db02"
      },
      "outputs": [],
      "source": [
        "# Looking tha attribute correlation with 'median_house_value'\n",
        "corr_matrix['median_house_value'].sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSjS-D6H6Io5"
      },
      "source": [
        "Correlation coefficient ranges from -1 to 1 (weak to strong correlation). When the coefficient is close to –1, it means that there is a strong negative correlation; it can be seen a small negative correlation between the latitude and the median house value (i.e., prices have a slight tendency to go down when you go north).\n",
        "\n",
        "Coefficients close to 0 means that there is no linear correlation between horizontal and vertical axes.\n",
        "\n",
        "**The correlation coefficient only measures linear correlations ('if x goes up, then y generally goes up/down').**\n",
        "\n",
        "\n",
        "Another way to check for correlation between attributes is to use the pandas `scatter_matrix()` function, which plots every numerical attribute against every other numerical attribute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1ANF4po6B1c"
      },
      "outputs": [],
      "source": [
        "from pandas.plotting import scatter_matrix\n",
        "attributes = ['median_house_value', 'median_income', 'total_rooms', 'housing_median_age']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 774
        },
        "id": "0QOG9Dh68I05",
        "outputId": "3f911374-0d3c-4442-8c3e-a6004fd89d31"
      },
      "outputs": [],
      "source": [
        "scatter_matrix(housing[attributes], figsize=(12,8))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWIu1Liad_l8"
      },
      "source": [
        "The most promising attribute to predict the median house value is the median income, so let’s zoom in on their correlation scatterplot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "id": "CZqJIV4P8P17",
        "outputId": "040b9534-4e85-4cb7-c87d-e6714b2f27f6"
      },
      "outputs": [],
      "source": [
        "housing.plot(kind='scatter', x='median_income', y='median_house_value', alpha=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tc1h6ZltehAd"
      },
      "source": [
        "This plot reveals a few things. First, the correlation is indeed very strong; the upward trend can clearly be seen, and the points are not too dispersed. Second, the price cap noticed earlier is clearly visible as a horizontal line at \\$500,000. But this plot reveals other less obvious straight lines: a horizontal line around \\$450,000, another around \\$350,000, perhaps one around \\$280,000, and a few more below that.\n",
        "\n",
        "To prevent the algorithms from learning to reproduce these data quirks, it may be neccesary to try removing the corresponding districts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OnXo_V-bWC7"
      },
      "source": [
        "# Experimenting with Attribute Combinations\n",
        "One last thing that may be wanted to be done before preparing the data for Machine Learninh algorithms is try out various attribute combinations. For example, the total numbers of rooms in a district is not very useful if the number of households is unknown. The number of rooms per household is the important data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgdajhG8eeKZ"
      },
      "outputs": [],
      "source": [
        "housing['rooms_per_household'] = housing['total_rooms']/housing['households']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFDoUStCcivq"
      },
      "outputs": [],
      "source": [
        "housing['bedrooms_per_room'] = housing['total_bedrooms']/housing['total_rooms']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDqf6wR4csIh"
      },
      "outputs": [],
      "source": [
        "housing['population_per_household'] = housing['population']/housing['households']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vkIUD1Sc7pL",
        "outputId": "41cb8a95-29a3-4ca4-c485-3f822a208c46"
      },
      "outputs": [],
      "source": [
        "# Look at the correlation matrix again:\n",
        "corr_matrix = housing.corr()\n",
        "corr_matrix['median_house_value'].sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwUGpCAsdUzI"
      },
      "source": [
        "The new `bedrooms_per_room` attribute is much more correlated with the median house value than the total number of rooms or bedrooms. Apparently, houses with a lower bedroom/ratio tend to be more expensive. The rooms_per_household is also more informative than the total_rooms attribute -the larger the houses, the more expensive they are-."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CZ-lfUwijfX"
      },
      "source": [
        "# Prepare the data for Machine Learning algorithm\n",
        "Instead of doing this manually, functions should be written for this purpose, for several good reasons:\n",
        "* This will allows to reproduce these transformations easily on any dataset (e.g., the next time a fresh dataset is getting).\n",
        "* Gradually a library of transformation functions can be built that can be reused in future projects.\n",
        "* These functions canb be used in the live system to transform the new data before feeding it to the algorithms.\n",
        "* This will make it possible to easily try various transformations and see which combination of transformations works best.\n",
        "\n",
        "But first, let's revert to a clean training set by copying `strat_train_set` once again. Also separate the predictors and the labels, since it doesn't  necessarily want to apply the same transformations to the predictors and the target values (note that drop() creates a copy of the data and does not affect strat_train_set)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3Pu_XOFdIeD"
      },
      "outputs": [],
      "source": [
        "housing = strat_train_set . drop ( 'median_house_value' , axis = 1 )\n",
        "housing_labels = strat_train_set [ 'median_house_value' ] . copy ()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLaACY9PkULb"
      },
      "source": [
        "# Data Cleaning\n",
        "Most Machine Learning algorithms cannot work with missing features, so let’s create a few functions to take care of them.\n",
        "\n",
        "The `total_bedrooms` attribute has some missing values, the options are:\n",
        "1. Get rid of the corresponding districts.\n",
        "2. Get rid of the whole attribute.\n",
        "3. Set the values to some value (zero, the mean, the median, etc.).\n",
        "\n",
        "These options can be easily accomplished by using the DataFrame's `dropna()`, `drop()`, and `fillna()` methods:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1NqTJRDj9J9"
      },
      "outputs": [],
      "source": [
        "housing.dropna (subset=['total_bedrooms']) # option 1\n",
        "housing.drop('total_bedrooms', axis=1) # option 2\n",
        "median = housing ['total_bedrooms'].median() # option 3\n",
        "housing['total_bedrooms'].fillna(median, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5hogn7ElduM"
      },
      "source": [
        "Scikit-Learn provides a handy class to take care of missing values: `SimpleImputer`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lmo_7KfolRmb"
      },
      "outputs": [],
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(strategy='median')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqbLBj_fln70"
      },
      "outputs": [],
      "source": [
        "# Since the median can only be computed on numerical attributes, it's neccesary to create a copy of the data without the text attribute ocean_proximity\n",
        "housing_num = housing.drop('ocean_proximity', axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "osTJXqATl1Bh",
        "outputId": "9338b31e-a0e4-4cc0-a5f6-fc06ef03dab7"
      },
      "outputs": [],
      "source": [
        "imputer.fit(housing_num)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fQ3v9WcmNTp"
      },
      "source": [
        "The `imputer` has simply computed the median of each attribute and stored the result in its `statistics_` instance variable. Only the `total_bedrooms` attribute had missing values, but we cannot be sure that there won’t be any missing values in new data after the system goes live, so it is safer to apply the `imputer` to all the numerical attributes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbSviTUhl5dy",
        "outputId": "115cfd60-6bb8-4c16-e3ec-7cdbfe13458f"
      },
      "outputs": [],
      "source": [
        "imputer.statistics_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDAL33K6mbA8",
        "outputId": "2e8109c0-e8b6-477c-b086-d0122660a60c"
      },
      "outputs": [],
      "source": [
        "housing_num.median().values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EBoq1G3mlcz"
      },
      "source": [
        "Now this “trained” imputer can be used to transform the training set by replacing missing values with the learned medians:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zPz4tkimfha"
      },
      "outputs": [],
      "source": [
        "X = imputer.transform(housing_num)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMuD2ku8mtMS"
      },
      "outputs": [],
      "source": [
        "# Put it back into a pandas DataFrame\n",
        "housing_tr = pd.DataFrame(X, columns=housing_num.columns, index=housing_num.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "gkdmn9tym8lT",
        "outputId": "e9cc72db-b72f-4216-bfed-426de301a801"
      },
      "outputs": [],
      "source": [
        "housing_tr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dq-0n4YrrLd"
      },
      "source": [
        "# Handling text and categorical attributes\n",
        "In this dataset, there is just one text attribute: the `ocean_proximity`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uABIk3JKm-fK"
      },
      "outputs": [],
      "source": [
        "housing_cat = housing[['ocean_proximity']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "DKGxxMr4sGr1",
        "outputId": "beaea39f-677e-4ed5-dc73-4b0a07801bb5"
      },
      "outputs": [],
      "source": [
        "housing_cat.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfZ8_sT7sLqM"
      },
      "source": [
        "Each of values represents a category. This is called a categorical attribute. Most ML algorithms prefer to work with numbers, so it's a good idea to convert these categories from text to numbers by using the `OrdinalEncoder` class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "725ojPirsJA7"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "ordinal_encoder = OrdinalEncoder()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zypIfp1spVw",
        "outputId": "a5711b3e-171a-446c-8262-f23ccbc918f5"
      },
      "outputs": [],
      "source": [
        "housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\n",
        "housing_cat_encoded[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t48aeqtZsy6K",
        "outputId": "cfd9e83d-9640-48ba-f022-1db7bb2b3f5c"
      },
      "outputs": [],
      "source": [
        "ordinal_encoder.categories_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xkw6k2Rqtrku"
      },
      "source": [
        "One issue with this representation is that ML algorithms will assume that two nearby values are more similar than two distant values. This may be fine in some cases (e.g., for ordered categories such as “bad,” “average,” “good,” and “excellent”), but it is obviously not the case for the ocean_proximity column (for example, categories 0 and 4 are clearly more similar than categories 0 and 1). To fix this issue, a common solution is to create one binary attribute per category: one attribute equal to 1 when the category is \"<1H OCEAN\" (and 0 otherwise), another attribute equal to 1 when the category is “INLAND” (and 0 otherwise), and so on. This is called one-hot encoding , because only one attribute will be equal to 1 (hot), while the others will be 0 (cold). The new attributes are sometimes called dummy attributes. Scikit-Learn provides a OneHotEncoder class to convert categorical values into one-hot vectors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwGyYfc-tT44",
        "outputId": "cb01676d-a55f-4596-fa55-396d4f9e33cb"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "cat_encoder = OneHotEncoder()\n",
        "housing_cat_1hot = cat_encoder.fit_transform(housing_cat)\n",
        "housing_cat_1hot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEEmtilUuYGk"
      },
      "source": [
        "The output is a SciPy sparse matrix, instead of a NumPy array. After one-hot encoding, we get a matrix with thousands of columns, and the matrix is full of 0s except for a single 1 per row. Using up tons of memory mostly to store zeros would be very wasteful, so instead a sparse matrix only stores the location of the nonzero elements.\n",
        "\n",
        "It can ben used mostly like a normal 2D array, but it can also be converted into a (dense) NumPy array:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TOSYL4TuR5D",
        "outputId": "c5c12d9a-3535-424d-a758-96ce8760b90b"
      },
      "outputs": [],
      "source": [
        "housing_cat_1hot.toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhG4pCoNvZpO",
        "outputId": "f4c6c11f-99c5-4777-c5ad-3cde0dd1f629"
      },
      "outputs": [],
      "source": [
        "cat_encoder . categories_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLdPVIkTvt3B"
      },
      "source": [
        "# Custom Transformers\n",
        "Although Scikit-Learn provides many useful transformers, it will need to write its own for tasks such as custom cleanup operations or combining specific attributes. It will want your transformer to work seamlessly with Scikit-Learn functionalities (such as pipelines), and since Scikit-Learn relies on duck typing (not inheritance), all you need to do is create a class and implement three methods: fit() (returning self ), transform() , and fit_transform() . You can get the last one for free by simply adding TransformerMixin as a base class. If you add BaseEstimator as a base class (and avoid *args and **kargs in your constructor), you will also get two extra methods ( get_params() and set_params() ) that will be useful for automatic hyperparameter tuning.\n",
        "\n",
        "\n",
        "Let's create a custom transformer to add extra attributes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9rpUZvvvgMA"
      },
      "outputs": [],
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "# column index\n",
        "rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6\n",
        "\n",
        "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, add_bedrooms_per_room=True): # no *args or **kargs\n",
        "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
        "    def fit(self, X, y=None):\n",
        "        return self  # nothing else to do\n",
        "    def transform(self, X):\n",
        "        rooms_per_household = X[:, rooms_ix] / X[:, households_ix]\n",
        "        population_per_household = X[:, population_ix] / X[:, households_ix]\n",
        "        if self.add_bedrooms_per_room:\n",
        "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
        "            return np.c_[X, rooms_per_household, population_per_household,\n",
        "                         bedrooms_per_room]\n",
        "        else:\n",
        "            return np.c_[X, rooms_per_household, population_per_household]\n",
        "\n",
        "attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\n",
        "housing_extra_attribs = attr_adder.transform(housing.values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fzme34i1w4ud"
      },
      "source": [
        "This hyperparameter will allow you to easily find out whether adding this attribute helps the Machine Learning algorithms or not. More generally, you can add a hyperparameter to gate any data preparation step that you are not 100% sure about. The more you automate these data preparation steps, the more combinations you can automatically try out, making it much more likely that you will find a great combination (and saving you a lot of time)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1csEhr5yQbM"
      },
      "source": [
        "Note that I hard coded the indices (3, 4, 5, 6) for concision and clarity in the book, but it would be much cleaner to get them dynamically, like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRJ864RYxr1b"
      },
      "outputs": [],
      "source": [
        "col_names = \"total_rooms\", \"total_bedrooms\", \"population\", \"households\"\n",
        "rooms_ix, bedrooms_ix, population_ix, households_ix = [\n",
        "    housing.columns.get_loc(c) for c in col_names] # get the column indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPVcO8_-xw2U"
      },
      "source": [
        "Also, housing_extra_attribs is a NumPy array, we've lost the column names (unfortunately, that's a problem with Scikit-Learn). To recover a DataFrame, you could run this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "n8vJrqEZxvKN",
        "outputId": "b658721a-7a51-4473-e213-5799d57a84dc"
      },
      "outputs": [],
      "source": [
        "housing_extra_attribs = pd.DataFrame(\n",
        "    housing_extra_attribs,\n",
        "    columns=list(housing.columns)+[\"rooms_per_household\", \"population_per_household\"],\n",
        "    index=housing.index)\n",
        "housing_extra_attribs.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZFNgUr-xXvn"
      },
      "source": [
        "# Feature Scaling\n",
        "With few exceptions, Machine Learning algorithms don’t perform well when the input numerical attributes have very different scales.\n",
        "\n",
        "There are two common ways to get all attributes to have the same scale: min-max scaling and standardization.\n",
        "\n",
        "* Min-max scaling (many people call this ***normalization*** ) is the simplest: values are shifted and rescaled so that they end up ranging from 0 to 1. We do this by subtracting the min value and dividing by the max minus the min. Scikit-Learn provides a transformer called MinMaxScaler for this. It has a feature_range hyperparameter that lets you change the range if, for some reason, you don’t want 0–1.\n",
        "* Standardization is different: first it subtracts the mean value (so standardized values always have a zero mean), and then it divides by the standard deviation so that the resulting distribution has unit variance. Unlike min-max scaling, standardization does not bound values to a specific range, which may be a problem for some algorithms (e.g., neural networks often expect an input value ranging from 0 to 1). However, **standardization is much less affected by outliers.** Scikit-Learn provides a transformer called `StandardScaler` for standardization .\n",
        "\n",
        "**WARNING!**\n",
        "As with all the transformations, it is important to fit the scalers to the training data only, not to the full dataset (including the test set). Only then can you use them to transform the training set and the test set (and new data)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsiUt_0Ezlur"
      },
      "source": [
        "#Transformation Pipelines\n",
        "As it's seen, there are many data transformation steps that need to be executed in the right order. Fortunately, Scikit-Learn provides the Pipeline class to help with such sequences of transformations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3HHejwv2zlEp"
      },
      "outputs": [],
      "source": [
        "# Small pipeline for the numerical attributes\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "num_pipeline = Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
        "        ('attribs_adder', CombinedAttributesAdder()),\n",
        "        ('std_scaler', StandardScaler()),\n",
        "    ])\n",
        "\n",
        "housing_num_tr = num_pipeline.fit_transform(housing_num)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiitMXrq0CdZ"
      },
      "source": [
        "The `Pipeline` constructor takes a list of name/estimator pairs defining a sequence of steps. **All but the last estimator must be transformers** (i.e., they must have a `fit_transform()` method). The names can be anything you like (as long as they are unique and don’t contain double underscores, __ ); they will come in handy later for hyperparameter tuning.\n",
        "\n",
        "When you call the pipeline’s `fit()` method, it calls fit_transform() `Inline code` sequentially on all transformers, passing the output of each call as the parameter to the next call until it reaches the final estimator, for which it calls the `fit()` method.\n",
        "\n",
        "The pipeline exposes the same methods as the final estimator. In this example, the last estimator is a `StandardScaler` , which is a transformer, so the pipeline has a `transform()` method that applies all the transforms to the data in sequence (and of course also a `fit_transform()` method, which is the one we used)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hW1hdqUcz344",
        "outputId": "d4eeee3b-f16a-4bb5-dfd7-2ebcddc73339"
      },
      "outputs": [],
      "source": [
        "housing_num_tr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-NA_9pK1lr-"
      },
      "source": [
        "Having a single transformer able to handle all columns would be more convenient. Fortunately, Sickit-Learn introduced the `ColumnTransformer` for this purpose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KRkoNs-W1ft4"
      },
      "outputs": [],
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "num_attribs = list(housing_num) # get the list of numerical column names\n",
        "cat_attribs = [\"ocean_proximity\"] # list of categorical column names\n",
        "\n",
        "full_pipeline = ColumnTransformer([\n",
        "        (\"num\", num_pipeline, num_attribs), # tuple: name, a transformer, and a list of names (or indices), returns a dense matrix\n",
        "        (\"cat\", OneHotEncoder(), cat_attribs), # OneHotEncoder retunrs a sparse matrix\n",
        "    ])\n",
        "\n",
        "housing_prepared = full_pipeline.fit_transform(housing) # ColumnTransformer applies each transformer to the appropiate columns and concatenates the outputs along the second axis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3xI3tjK5sHW",
        "outputId": "a51e6f6b-a350-4dab-83ee-d17001fe31c5"
      },
      "outputs": [],
      "source": [
        "housing_prepared"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_Yaf7dw5vhX",
        "outputId": "7a3bb165-58cd-4bd4-8760-c12acdee3384"
      },
      "outputs": [],
      "source": [
        "housing_prepared.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CM6jKio3370F"
      },
      "source": [
        "This is how we have a preprocessing pipeline that takes the full housing data and applies the appropriate transformations to each column."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15uwbx8k5j4J"
      },
      "source": [
        "**TIP**\n",
        "\n",
        "Instead of using a transformer, you can specify the string \"`drop`\" if you want the columns to be dropped, or you can specify \"`passthrough`\" if you want the columns to be left untouched. By default, the remaining columns (i.e., the ones that were not listed) will be dropped, but you can set the remainder hyperparameter to any transformer (or to \"`passthrough`\") if you want these columns to be handled differently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gn0MrMv6zCJ"
      },
      "source": [
        "# Select and train a model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWUWC9K1BKR3"
      },
      "source": [
        "## Train a Linear Regression model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjmbI8vM7Tl2",
        "outputId": "824f2fdb-79d4-481f-daa3-8c547f8bbc8c"
      },
      "outputs": [],
      "source": [
        "housing_prepared"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DopJNct07Wdu",
        "outputId": "399c6ea9-aa39-438c-f2d4-515911a13bc8"
      },
      "outputs": [],
      "source": [
        "housing_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "1n_Vcf8x36dJ",
        "outputId": "f333b1cd-d512-4c8b-9cde-3bb038cc8f98"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(housing_prepared, housing_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJ-g8r265uhe",
        "outputId": "a6638dba-f79a-43cf-c822-05f9bfba7c1f"
      },
      "outputs": [],
      "source": [
        "# let's try the full preprocessing pipeline on a few training instances\n",
        "some_data = housing.iloc[:5]\n",
        "some_labels = housing_labels.iloc[:5]\n",
        "some_data_prepared = full_pipeline.transform(some_data)\n",
        "\n",
        "print(\"Predictions:\", lin_reg.predict(some_data_prepared))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ojPOWYj7pya"
      },
      "source": [
        "Compare against the actual values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dlwZY4t7luP",
        "outputId": "663fc16a-2431-40e6-cdb0-b4f8969721c6"
      },
      "outputs": [],
      "source": [
        "print(\"Labels:\", list(some_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98Q9F1N98PsV"
      },
      "source": [
        "The predictions are not exactly accurate (e.g., the first prediction is off by close to 40%!). Let’s measure this regression model’s RMSE on the whole training set using Scikit-Learn’s `mean_squared_error()` function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U27ecwP074WF",
        "outputId": "b26bc805-3938-4bf8-c654-7403d83f50d1"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "housing_predictions = lin_reg.predict(housing_prepared)\n",
        "lin_mse = mean_squared_error(housing_labels, housing_predictions)\n",
        "lin_rmse = np.sqrt(lin_mse)\n",
        "lin_rmse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nbiA54q8sjL"
      },
      "source": [
        "Most districts’ **median_housing_values** range between \\$120,000 and \\$265,000, so a typical prediction error of \\$68,628 is not very satisfying. **This is an example of a model underfitting the training data.**\n",
        "\n",
        "When this happens it can mean that the features do not provide enough information to make good predictions, or that the model is not powerful enough.\n",
        "\n",
        "The main ways to fix underfitting are to select a more powerful model, to feed the training algorithm with better features, or to reduce the constraints on the model. This model is not regularized, which rules out the last option. You could try to add more features (e.g., the log of the population), but first let’s try a more complex model to see how it does.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rAIPdgBBv_2"
      },
      "source": [
        "**Note**: since Scikit-Learn 0.22, you can get the RMSE directly by calling the mean_squared_error() function with squared=False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35QnE4HSBt2Q",
        "outputId": "45ba2052-5ebc-49f7-b62f-2472562bbf59"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "lin_mae = mean_absolute_error(housing_labels, housing_predictions)\n",
        "lin_mae"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xqhtq5TnCL_8"
      },
      "source": [
        "## Train a DecisionTreeRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "wM_lMoND8hTP",
        "outputId": "da746d19-fd80-4a85-cf55-6bd454e595a8"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "tree_reg = DecisionTreeRegressor(random_state=42)\n",
        "tree_reg.fit(housing_prepared, housing_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACSAKp20CVg1",
        "outputId": "77c41d8e-d56b-41a9-be0a-5c570a1c7c7f"
      },
      "outputs": [],
      "source": [
        "housing_predictions = tree_reg.predict(housing_prepared)\n",
        "tree_mse = mean_squared_error(housing_labels, housing_predictions)\n",
        "tree_rmse = np.sqrt(tree_mse)\n",
        "tree_rmse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSY7_nfwCmp6"
      },
      "source": [
        "Having no error at this time, it is much more likely that **the model has badly overfit the data.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSg2R5jzDD2G"
      },
      "source": [
        "# Better Evaluation Using Cross-Validation\n",
        "The following code randomly splits the training set into 10 distinct subsets called folds , then it trains and evaluates the Decision Tree model 10 times, picking a different fold for evaluation every time and training on the other 9 folds. The result is an array containing the 10 evaluation scores:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_MhEwKEChQY"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "scores = cross_val_score(tree_reg, housing_prepared, housing_labels,\n",
        "                         scoring=\"neg_mean_squared_error\", cv=10)\n",
        "tree_rmse_scores = np.sqrt(-scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZ9gkl4eEoyj"
      },
      "source": [
        "**Warning**\n",
        "\n",
        "Scikit-Learn’s cross-validation features expect a utility function (greater is better) rather than a cost function (lower is better), so the scoring function is actually the opposite of the MSE (i.e., a negative value), which is why the preceding code computes -scores before calculating the square root."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeQccotPEGRb",
        "outputId": "53dd932b-3121-4517-9058-bd3364f1792c"
      },
      "outputs": [],
      "source": [
        "def display_scores(scores):\n",
        "    print(\"Scores:\", scores)\n",
        "    print(\"Mean:\", scores.mean())\n",
        "    print(\"Standard deviation:\", scores.std())\n",
        "\n",
        "display_scores(tree_rmse_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31PkE1wnFvMF"
      },
      "source": [
        "Now the Decision Tree doesn’t look as good as it did earlier. In fact, it seems to perform worse than the Linear Regression model! Notice that cross-validation allows you to get not only an estimate of the performance of your model, but also a measure of how precise this estimate is (i.e., its standard deviation). The Decision Tree has a score of approximately 71,407, generally ±2,439. You would not have this information if you just used one validation set. But cross-validation comes at the cost of training the model several times, so it is not always possible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YpayXjhwE49a",
        "outputId": "bf9ef22c-8afc-462a-ec62-37fa82a44189"
      },
      "outputs": [],
      "source": [
        "# Computing the same scores for the Linear Regression model\n",
        "lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels,\n",
        "                             scoring=\"neg_mean_squared_error\", cv=10)\n",
        "lin_rmse_scores = np.sqrt(-lin_scores)\n",
        "display_scores(lin_rmse_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQacHHFoGOau"
      },
      "source": [
        "The Decision Tree model is overfitting so badly that it performs worse than the Linear Regression model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7MaJHREGbE0"
      },
      "source": [
        "## Train RandomForestRegressor\n",
        "Random Forests work by training many Decision Trees on random subsets of the features, then averaging out their predictions. Building a model on top of many other models is called **Ensemble Learning**, and it is often a great way to push ML algorithms even further."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "Ay0js-KzGJkz",
        "outputId": "fbf3072d-6fef-4dfb-e799-c106ee1abaf4"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "forest_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "forest_reg.fit(housing_prepared, housing_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jgJOVu1HOEp",
        "outputId": "dcc13cfa-0986-4f3b-d07d-b3a166571585"
      },
      "outputs": [],
      "source": [
        "housing_predictions = forest_reg.predict(housing_prepared)\n",
        "forest_mse = mean_squared_error(housing_labels, housing_predictions)\n",
        "forest_rmse = np.sqrt(forest_mse)\n",
        "forest_rmse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aKcZTJ2HTPA",
        "outputId": "fba93096-1fbd-4c0a-d642-d5d6c1a47bd0"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "forest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels,\n",
        "                                scoring=\"neg_mean_squared_error\", cv=10)\n",
        "forest_rmse_scores = np.sqrt(-forest_scores)\n",
        "display_scores(forest_rmse_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcOWuu8CJbrP",
        "outputId": "01c66514-7508-44ea-8502-33a9ddf1aa9d"
      },
      "outputs": [],
      "source": [
        "scores = cross_val_score(lin_reg, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv=10)\n",
        "pd.Series(np.sqrt(-scores)).describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGgJ2wmkHtdO"
      },
      "source": [
        "Random Forests look very promising. However, note that the score on the training set is still much lower than on the validation sets, meaning that the model is still overfitting the training set. Possible solutions for overfitting are to simplify the model, constrain it (i.e., regularize it), or get a lot more training data.\n",
        "\n",
        "Before dive much deeper into Random Forest, you should try out many other models from various categories of ML algorithms. The goal is to shortlist a few (two to five) promising models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyOUmOrWH0eG"
      },
      "source": [
        "## Saving the models\n",
        "You should save every model you experiment with so that you can come back easily to any model you want. Is necessary to save:\n",
        "* Both the hyperparameters and the trained parameters.\n",
        "* The cross-validation scores.\n",
        "* Perhaps actual predictions as well.\n",
        "\n",
        "To save the models, the Python's `pickle` module or `joblib` library can be used, which is more efficient at serializing large NumPy arrays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUD0L1IyHb_Q"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "joblib.dump(forest_reg, \"my_model.pkl\")\n",
        "# and later...\n",
        "my_model_loaded = joblib.load(\"my_model.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WOV_HkGXVjJ",
        "outputId": "1f37c4df-5c6a-466f-ba97-4add002b5d21"
      },
      "outputs": [],
      "source": [
        "print(\"Predictions:\\t\", my_model_loaded.predict(housing_prepared))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "02_end_to_end_machine_learning_project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
